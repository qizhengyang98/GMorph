{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122071f6-1618-45c6-b491-5b6d8c1eb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a373d793-f544-46ba-9a90-bb15b40c1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a9e94f-dd26-46da-8119-82585f5dbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a858cefc-c7aa-468d-828e-bffa8f7feb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config_1 = AutoConfig.from_pretrained(\"../transformer_model/cola/checkpoint\")\n",
    "# bert_config_2 = AutoConfig.from_pretrained(\"../transformer_model/sst2/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255ac681-2080-4781-be15-5eda0e3231a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config_1.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65074a96-e35d-46d2-aed4-b2fa04290560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config_2.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa556025-80ac-4c3f-8d8f-0fe753db5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(config.hidden_size / config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ca13e9-30b9-4ce3-8a66-0a86656a4db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from benchmark_glue import glue_origin, glue_SA_t002\n",
    "\n",
    "model_1 = glue_origin().eval().to(DEVICE)\n",
    "model_2 = glue_SA_t002().eval().to(DEVICE)\n",
    "sample_input = torch.ones((1,128), dtype=torch.int).to(DEVICE)\n",
    "\n",
    "import timeit\n",
    "def test_latency(model, input_x, timing_number=30, timing_repeat=30):\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(input_x)\n",
    "\n",
    "    latency = (\n",
    "            np.array(timeit.Timer(lambda:model(input_x)).repeat(repeat=timing_repeat, number=timing_number))\n",
    "            * 1000\n",
    "            / timing_number\n",
    "        )\n",
    "    latency = {\"mean\": np.mean(latency), \"median\": np.median(latency), \"std\":np.std(latency)}\n",
    "\n",
    "    print(\"Inference Latency: %s\" % (latency))\n",
    "    return latency['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6491ed27-778a-4583-ae4e-3067278225bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(sample_input)\n",
    "# import time\n",
    "# s = time.time()\n",
    "# model(sample_input)\n",
    "# e = time.time()\n",
    "# print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06941b4-1ca0-418e-8202-fc09bb388d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, param in model.task2.named_parameters():\n",
    "#     print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1745e9a-1775-4595-85e7-732c13aef726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 71.17642154999582, 'median': 71.08059986688507, 'std': 0.30839331829098804}\n",
      "Inference Latency: {'mean': 37.73031043448201, 'median': 37.742419550098326, 'std': 0.11676270511608507}\n",
      "1.8864520522191932\n"
     ]
    }
   ],
   "source": [
    "# test_latency(model, sample_input)\n",
    "latency1 = test_latency(model_1, sample_input)\n",
    "latency2 = test_latency(model_2, sample_input)\n",
    "print(latency1/latency2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186b60c8-5f67-4edb-b3ac-85fa094fad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, [sample_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56861bfc-69bd-4b2c-8ec3-f98b5a130730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "trt_model = torch_tensorrt.compile(traced_model,\n",
    "                                  inputs=[torch_tensorrt.Input(shape=[1,128], dtype=torch.int32),],\n",
    "                                  enabled_precisions={torch.float32},\n",
    "                                  truncate_long_and_double=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01dcd7-905d-405f-9cdd-7053373b858d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b3709c-3b56-4f82-bc3d-dcd30d3aba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_vit import vit_origin, vit_SA_t002\n",
    "\n",
    "model_1 = vit_origin().eval().to(DEVICE)\n",
    "model_2 = vit_SA_t002().eval().to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "\n",
    "import timeit\n",
    "def test_latency(model, input_x, timing_number=30, timing_repeat=30):\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(input_x)\n",
    "\n",
    "    latency = (\n",
    "            np.array(timeit.Timer(lambda:model(input_x)).repeat(repeat=timing_repeat, number=timing_number))\n",
    "            * 1000\n",
    "            / timing_number\n",
    "        )\n",
    "    latency = {\"mean\": np.mean(latency), \"median\": np.median(latency), \"std\":np.std(latency)}\n",
    "\n",
    "    print(\"Inference Latency: %s\" % (latency))\n",
    "    return latency['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e5bccb-aac9-44b6-a15f-5d3d4ef08dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06303930282592773\n"
     ]
    }
   ],
   "source": [
    "# model(sample_input)\n",
    "# import time\n",
    "# s = time.time()\n",
    "# model(sample_input)\n",
    "# e = time.time()\n",
    "# print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6041692-7953-45a5-b3e6-b470bbdd746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 62.96257890110913, 'median': 62.892086883463584, 'std': 0.23447427437433488}\n",
      "Inference Latency: {'mean': 21.30385106778704, 'median': 21.29944031670069, 'std': 0.030799350123714722}\n",
      "2.9554552696020813\n"
     ]
    }
   ],
   "source": [
    "latency1 = test_latency(model_1, sample_input)\n",
    "latency2 = test_latency(model_2, sample_input)\n",
    "print(latency1/latency2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e017a3d-3349-44d3-8862-c468ed6d683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qizhengyang_umass_edu/MTL_compiler/test_metamorph/transformers/src/transformers/models/vit/modeling_vit.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/qizhengyang_umass_edu/MTL_compiler/test_metamorph/transformers/src/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(model, [sample_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe6c48-cb31-4b27-bcbd-4218b9ed1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "trt_model = torch_tensorrt.compile(traced_model,\n",
    "                                  inputs=[torch_tensorrt.Input(shape=[1,3,224,224], dtype=torch.float32),],\n",
    "                                  enabled_precisions={torch.float32},\n",
    "                                  truncate_long_and_double=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44e0c9-316a-4374-a12b-1328af502c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a688e83a-5960-4c7b-8fb7-a6be00842d80",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported value kind: Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m torch_executed_ops \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     22\u001b[0m ]\n\u001b[0;32m---> 24\u001b[0m optimized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_tensorrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorchscript\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43menabled_precisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_precisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkspace_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_block_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_executed_ops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_executed_ops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(optimized_model)\u001b[39;00m\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch_tensorrt/_compile.py:132\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(module, ir, inputs, enabled_precisions, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;241m==\u001b[39m _ModuleType\u001b[38;5;241m.\u001b[39mnn:\n\u001b[1;32m    128\u001b[0m         logging\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    129\u001b[0m             logging\u001b[38;5;241m.\u001b[39mLevel\u001b[38;5;241m.\u001b[39mInfo,\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule was provided as a torch.nn.Module, trying to script the module with torch.jit.script. In the event of a failure please preconvert your module to TorchScript\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m         )\n\u001b[0;32m--> 132\u001b[0m         ts_mod \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_tensorrt\u001b[38;5;241m.\u001b[39mts\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    134\u001b[0m         ts_mod, inputs\u001b[38;5;241m=\u001b[39minputs, enabled_precisions\u001b[38;5;241m=\u001b[39menabled_precisions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_ir \u001b[38;5;241m==\u001b[39m _IRType\u001b[38;5;241m.\u001b[39mfx:\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1283\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 546\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m/work/qizhengyang_umass_edu/.conda/envs/tvm-build/lib/python3.10/site-packages/torch/jit/_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    394\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    395\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 397\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported value kind: Tensor"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "torch_tensorrt.set_device(0)\n",
    "# Enabled precision for TensorRT optimization\n",
    "enabled_precisions = {torch.float}\n",
    "\n",
    "# Whether to print verbose logs\n",
    "debug = False\n",
    "\n",
    "# Workspace size for TensorRT\n",
    "workspace_size = 20 << 30\n",
    "\n",
    "# Maximum number of TRT Engines\n",
    "# (Lower value allows more graph segmentation)\n",
    "min_block_size = 1\n",
    "\n",
    "# Operations to Run in Torch, regardless of converter support\n",
    "torch_executed_ops = {}\n",
    "\n",
    "inputs = [\n",
    "    torch.ones((1,128), dtype=torch.int).to(DEVICE)\n",
    "]\n",
    "\n",
    "optimized_model = torch_tensorrt.compile(\n",
    "    model,\n",
    "    'torchscript',\n",
    "    inputs,\n",
    "    enabled_precisions=enabled_precisions,\n",
    "    debug=debug,\n",
    "    workspace_size=workspace_size,\n",
    "    min_block_size=min_block_size,\n",
    "    torch_executed_ops=torch_executed_ops,\n",
    ")\n",
    "\n",
    "# print(optimized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d24bdd9-ef2f-41e9-82e8-b8e850d376f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008757114410400391\n"
     ]
    }
   ],
   "source": [
    "optimized_model(sample_input)\n",
    "import time\n",
    "s = time.time()\n",
    "optimized_model(sample_input)\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e772d212-1089-4d3d-8b60-b2d8b3a6e918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 6.508342187929277, 'median': 6.4912418330398705, 'std': 0.04736630698576426}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.508342187929277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_latency(optimized_model, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123eafe4-6be3-4adb-b01a-0131015ce304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 28.4033805856921, 'median': 28.52801685027468, 'std': 0.4700573073406354}\n",
      "Inference Latency: {'mean': 5.598582867726994, 'median': 5.597151183368018, 'std': 0.051909800877829426}\n",
      "Inference Latency: {'mean': 9.847025916678831, 'median': 9.842854498613935, 'std': 0.04805790047645195}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import tensorrt\n",
    "# import torch_tensorrt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet34, vgg16\n",
    "from benchmark_scene_2 import scene_origin, scene_SA000, scene_SA001, scene_LC001, scene_SA002, scene_LCR002, scene_SA001_test\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model1 = vgg16().eval().to(DEVICE)\n",
    "model2 = resnet18().eval().to(DEVICE)\n",
    "model3 = resnet34().eval().to(DEVICE)\n",
    "\n",
    "# model = scene_origin().eval().to(DEVICE)\n",
    "\n",
    "# model1 = scene_SA000().eval().to(DEVICE)\n",
    "# model2 = scene_SA001_test().eval().to(DEVICE)\n",
    "# model3 = scene_SA001().eval().to(DEVICE)\n",
    "# model = scene_LC001().eval().to(DEVICE)\n",
    "\n",
    "# model = scene_SA002().eval().to(DEVICE)\n",
    "# model = scene_LCR002().eval().to(DEVICE)\n",
    "\n",
    "sample_input = torch.rand(16,3,224,224).to(DEVICE) \n",
    "# when input size is (1,3,224,224), vgg16=3.67ms, resnet18=2.98ms, resnet34=5.12ms\n",
    "# when input size is (16,3,224,224), vgg16=28.62ms, resnet18=5.69ms, resnet34=9.92ms\n",
    "\n",
    "import timeit\n",
    "def test_trt_latency(trt_model, input_x, timing_number=30, timing_repeat=30):\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "        _ = trt_model(input_x)\n",
    "\n",
    "    latency = (\n",
    "            np.array(timeit.Timer(lambda:trt_model(input_x)).repeat(repeat=timing_repeat, number=timing_number))\n",
    "            * 1000\n",
    "            / timing_number\n",
    "        )\n",
    "    latency = {\"mean\": np.mean(latency), \"median\": np.median(latency), \"std\":np.std(latency)}\n",
    "\n",
    "    print(\"Inference Latency: %s\" % (latency))\n",
    "    return latency['mean']\n",
    "\n",
    "def test_trt_latency_cuda(trt_model, input_x, timing_repeat=30, timing_number=30):\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings = np.zeros((timing_repeat, timing_number))\n",
    "    timing_mean, timing_std = None, None\n",
    "\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "        _ = trt_model(input_x)\n",
    "    \n",
    "    while timing_mean is None or timing_std > timing_mean/10:\n",
    "        # Measure Performance\n",
    "        with torch.no_grad():\n",
    "            for i in range(timing_repeat):\n",
    "                for j in range(timing_number):\n",
    "                    torch.manual_seed(i+j)\n",
    "                    start.record()\n",
    "                    _ = trt_model(input_x)\n",
    "                    end.record()\n",
    "                    # Wait for GPU synchronization\n",
    "                    torch.cuda.synchronize()\n",
    "                    timings[i][j] = start.elapsed_time(end)\n",
    "        \n",
    "        timing_mean = np.mean(np.mean(timings, axis=1))\n",
    "        timing_median = np.median(timings)\n",
    "        timing_std = np.mean(np.std(timings, axis=1, ddof=1))\n",
    "    \n",
    "    latency = {\"mean\": timing_mean, \"median\": timing_median, \"std\": timing_std}\n",
    "    print(\"Inference Latency: %s \\n\" % (latency))\n",
    "    return latency['mean']\n",
    "\n",
    "# trt_module = torch_tensorrt.compile(\n",
    "#         model,\n",
    "#         inputs=[\n",
    "#             torch_tensorrt.Input(sample_input.shape),\n",
    "#         ],\n",
    "#         min_block_size=1\n",
    "#     )\n",
    "# print(trt_module)\n",
    "\n",
    "# test_trt_latency(model, sample_input)\n",
    "test_trt_latency(model1, sample_input)\n",
    "test_trt_latency(model2, sample_input)\n",
    "test_trt_latency(model3, sample_input)\n",
    "# print('------------------')\n",
    "\n",
    "# test_trt_latency_cuda(model, sample_input)\n",
    "# test_trt_latency_cuda(model1, sample_input)\n",
    "# test_trt_latency_cuda(model2, sample_input)\n",
    "# test_trt_latency_cuda(model3, sample_input)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f861ecfd-64bb-431a-ac0c-b0ebd469e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 9.40445987595452, 'median': 9.372672080993652, 'std': 0.1024116841411341} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 9.432496150334677, 'median': 9.400320053100586, 'std': 0.12411760911061172} \n",
      "\n",
      "Inference Latency: {'mean': 4.286457173559401, 'median': 4.261888027191162, 'std': 0.09461355343077846} \n",
      "\n",
      "Inference Latency: {'mean': 4.509996201197307, 'median': 4.488192081451416, 'std': 0.10305212129050935} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.509996201197307"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from benchmark_scene_2 import scene_origin, scene_SA000, scene_SA001, scene_LC001, scene_SA002, scene_LCR002\n",
    "\n",
    "model1 = scene_origin().eval().to(DEVICE)\n",
    "\n",
    "model2 = scene_SA000().eval().to(DEVICE)\n",
    "\n",
    "model3 = scene_SA001().eval().to(DEVICE)\n",
    "# model3 = scene_LC001().eval().to(DEVICE)\n",
    "\n",
    "model2 = scene_SA002().eval().to(DEVICE)\n",
    "model3 = scene_LCR002().eval().to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "\n",
    "from metamorph.metrics.testing_utils import test_latency, test_latency_cuda\n",
    "test_latency_cuda(model1, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "test_latency_cuda(model3, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad289c-5e12-45b9-922b-045d5163539e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6371308a-681f-4eb8-8852-be3d935ceb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/qizhengyang_umass_edu/.conda/envs/morph/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 13.383713318035007, 'median': 12.48376142854492, 'std': 4.892506056394417} \n",
      "\n",
      "Inference Latency: {'mean': 12.680946503745185, 'median': 12.679167747497559, 'std': 0.05241888532658609} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 12.787769234975178, 'median': 12.773248195648193, 'std': 0.05217521855988129} \n",
      "\n",
      "Inference Latency: {'mean': 8.701213091744316, 'median': 8.699999809265137, 'std': 0.036917118554504254} \n",
      "\n",
      "Inference Latency: {'mean': 8.764629673428006, 'median': 8.755423545837402, 'std': 0.02770630065376952} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 12.870020713806154, 'median': 12.846447944641113, 'std': 0.057231621607244126} \n",
      "\n",
      "Inference Latency: {'mean': 8.74160758336385, 'median': 8.738831996917725, 'std': 0.03358236794498908} \n",
      "\n",
      "Inference Latency: {'mean': 8.805317924287582, 'median': 8.790016174316406, 'std': 0.05643920699069443} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 12.913941161897446, 'median': 12.89846420288086, 'std': 0.04646359764267928} \n",
      "\n",
      "Inference Latency: {'mean': 8.74997865041097, 'median': 8.740863800048828, 'std': 0.04419586808887401} \n",
      "\n",
      "Inference Latency: {'mean': 8.805844071706135, 'median': 8.79308795928955, 'std': 0.0303709590514786} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.805844071706135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from benchmark_face16 import face16_origin, face16_SA_002, face16_SA_002_2\n",
    "model1 = face16_origin().eval().to(DEVICE)\n",
    "model2 = face16_SA_002().eval().to(DEVICE)\n",
    "model3 = face16_SA_002_2().eval().to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "\n",
    "from metamorph.metrics.testing_utils import test_latency, test_latency_cuda\n",
    "\n",
    "test_latency(model1, sample_input)\n",
    "test_latency_cuda(model1, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "test_latency_cuda(model3, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "test_latency_cuda(model3, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "test_latency_cuda(model3, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b592dbdc-336b-4710-81d3-2e3a3c1722bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/qizhengyang_umass_edu/.conda/envs/morph/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 1.8515058090755094, 'median': 1.3792970508802682, 'std': 2.5211677624682367} \n",
      "\n",
      "Inference Latency: {'mean': 1.5285076256593075, 'median': 1.56876802444458, 'std': 0.038116534532484286} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 1.5719670055972208, 'median': 1.5503360033035278, 'std': 0.05123392398741388} \n",
      "\n",
      "Inference Latency: {'mean': 1.4167135990990531, 'median': 1.415168046951294, 'std': 0.023498019636416494} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 1.2646313251389396, 'median': 1.2472319602966309, 'std': 0.04322277833653522} \n",
      "\n",
      "Inference Latency: {'mean': 1.405659947792689, 'median': 1.3967360258102417, 'std': 0.021714805294772148} \n",
      "\n",
      "\n",
      "Inference Latency: {'mean': 1.24925874127282, 'median': 1.240064024925232, 'std': 0.03346458182534417} \n",
      "\n",
      "Inference Latency: {'mean': 1.4111106843418542, 'median': 1.395695984363556, 'std': 0.028712020964693184} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4111106843418542"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from benchmark_toy import toy_best_LC_norule, toy_random_compare_SA\n",
    "model1 = toy_best_LC_norule().eval().to(DEVICE)\n",
    "model2 = toy_random_compare_SA().eval().to(DEVICE)\n",
    "sample_input = torch.rand(1,1,48,48).to(DEVICE)\n",
    "\n",
    "from metamorph.metrics.testing_utils import test_latency, test_latency_cuda\n",
    "\n",
    "test_latency(model1, sample_input)\n",
    "test_latency_cuda(model1, sample_input)\n",
    "\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)\n",
    "print()\n",
    "test_latency_cuda(model1, sample_input)\n",
    "test_latency_cuda(model2, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb35181-8741-477e-b411-8c672a2371a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m1h\u001b=\n",
      "Currently Loaded Modules:\u001b[m\n",
      "  1) miniconda/4.11.0   3) mathematica/12.2.0\u001b[m\n",
      "  2) matlab/r2021a      4) julia/1.7.2\u001b[m\n",
      "\u001b[m\n",
      " \u001b[m\n",
      "\u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "# !module load cuda/11.3.1\n",
    "# !module load cudnn/cuda11-8.4.1.50\n",
    "# !module list\n",
    "# !which sh\n",
    "# !source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a688c1-8f04-4700-8438-672388bc56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import results_toymodel, results_scene, results_face16, results_face\n",
    "\n",
    "all_model_list = ['origin', 'merge_half_conv', 'merge_all_conv', 'SA', 'LC_wo_rule', 'LC_w_rule']\n",
    "\n",
    "# model = results_toymodel[all_model_list[0]].to(DEVICE)\n",
    "# model = results_scene[all_model_list[0]].to(DEVICE)\n",
    "# model = results_face16[all_model_list[0]].to(DEVICE)\n",
    "# model = results_face[all_model_list[0]].to(DEVICE)\n",
    "# print(model)\n",
    "\n",
    "# origin_model = results_toymodel['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_toymodel['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_toymodel['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_toymodel['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_toymodel['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_toymodel['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_scene['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_scene['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_scene['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_scene['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_scene['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_scene['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "origin_model = results_face16['origin'].to(DEVICE)\n",
    "merge_half_conv_model = results_face16['merge_half_conv'].to(DEVICE)\n",
    "merge_all_conv_model = results_face16['merge_all_conv'].to(DEVICE)\n",
    "SA_model = results_face16['SA'].to(DEVICE)\n",
    "LC_wo_rule_model = results_face16['LC_wo_rule'].to(DEVICE)\n",
    "LC_w_rule_model = results_face16['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_face['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_face['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_face['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_face['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_face['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_face['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# sample_input = torch.rand(1,1,48,48).to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "# print(model(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7c2a6-95ff-4cff-a3ea-e7f06b29ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "from benchmark import results_toymodel_t0, results_scene_t0, results_face16_t0, results_face_t0\n",
    "\n",
    "all_model_list = ['origin', 'merge_half_conv', 'merge_all_conv', 'SA', 'LC_wo_rule', 'LC_w_rule']\n",
    "\n",
    "# model = results_toymodel_t0[all_model_list[0]].to(DEVICE)\n",
    "model = results_scene_t0[all_model_list[0]].to(DEVICE)\n",
    "# model = results_face16_t0[all_model_list[0]].to(DEVICE)\n",
    "# model = results_face_t0[all_model_list[0]].to(DEVICE)\n",
    "# print(model)\n",
    "\n",
    "# origin_model = results_toymodel_t0['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_toymodel_t0['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_toymodel_t0['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_toymodel_t0['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_toymodel_t0['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_toymodel_t0['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "origin_model = results_scene_t0['origin'].to(DEVICE)\n",
    "merge_half_conv_model = results_scene_t0['merge_half_conv'].to(DEVICE)\n",
    "merge_all_conv_model = results_scene_t0['merge_all_conv'].to(DEVICE)\n",
    "SA_model = results_scene_t0['SA'].to(DEVICE)\n",
    "LC_wo_rule_model = results_scene_t0['LC_wo_rule'].to(DEVICE)\n",
    "LC_w_rule_model = results_scene_t0['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_face16_t0['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_face16_t0['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_face16_t0['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_face16_t0['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_face16_t0['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_face16_t0['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_face_t0['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_face_t0['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_face_t0['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_face_t0['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_face_t0['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_face_t0['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# sample_input = torch.rand(1,1,48,48).to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "# print(model(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a767ea-f7d9-453f-9541-000c1f1f11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8504a0-f867-490f-bad7-14adae82b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import results_toymodel_t002, results_scene_t002, results_face16_t002, results_face_t002\n",
    "\n",
    "all_model_list = ['origin', 'merge_half_conv', 'merge_all_conv', 'SA', 'LC_wo_rule', 'LC_w_rule']\n",
    "\n",
    "# model = results_toymodel_t002[all_model_list[0]].to(DEVICE)\n",
    "# model = results_scene_t002[all_model_list[0]].to(DEVICE)\n",
    "# model = results_face16_t002[all_model_list[0]].to(DEVICE)\n",
    "model = results_face_t002[all_model_list[0]].to(DEVICE)\n",
    "# print(model)\n",
    "\n",
    "# origin_model = results_toymodel_t002['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_toymodel_t002['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_toymodel_t002['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_toymodel_t002['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_toymodel_t002['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_toymodel_t002['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_scene_t002['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_scene_t002['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_scene_t002['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_scene_t002['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_scene_t002['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_scene_t002['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# origin_model = results_face16_t002['origin'].to(DEVICE)\n",
    "# merge_half_conv_model = results_face16_t002['merge_half_conv'].to(DEVICE)\n",
    "# merge_all_conv_model = results_face16_t002['merge_all_conv'].to(DEVICE)\n",
    "# SA_model = results_face16_t002['SA'].to(DEVICE)\n",
    "# LC_wo_rule_model = results_face16_t002['LC_wo_rule'].to(DEVICE)\n",
    "# LC_w_rule_model = results_face16_t002['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "origin_model = results_face_t002['origin'].to(DEVICE)\n",
    "merge_half_conv_model = results_face_t002['merge_half_conv'].to(DEVICE)\n",
    "merge_all_conv_model = results_face_t002['merge_all_conv'].to(DEVICE)\n",
    "SA_model = results_face_t002['SA'].to(DEVICE)\n",
    "LC_wo_rule_model = results_face_t002['LC_wo_rule'].to(DEVICE)\n",
    "LC_w_rule_model = results_face_t002['LC_w_rule'].to(DEVICE)\n",
    "\n",
    "# sample_input = torch.rand(1,1,48,48).to(DEVICE)\n",
    "sample_input = torch.rand(1,3,224,224).to(DEVICE)\n",
    "# print(model(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33640d8-ad0a-4f89-9d73-1786cf81c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metamorph.metrics.testing_utils import test_latency, test_latency_cuda\n",
    "\n",
    "# test_latency(model, sample_input)\n",
    "# test_latency_cuda(model, sample_input)\n",
    "\n",
    "import timeit\n",
    "def test_latency(model, input_x, timing_number=30, timing_repeat=30):\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(input_x)\n",
    "\n",
    "    latency = (\n",
    "            np.array(timeit.Timer(lambda:model(input_x)).repeat(repeat=timing_repeat, number=timing_number))\n",
    "            * 1000\n",
    "            / timing_number\n",
    "        )\n",
    "    latency = {\"mean\": np.mean(latency), \"median\": np.median(latency), \"std\":np.std(latency)}\n",
    "\n",
    "    print(\"Inference Latency: %s\" % (latency))\n",
    "    return latency['mean']\n",
    "\n",
    "def test_latency_cuda(cmp_graph, dummy_input, timing_repeat=30, timing_number=30):\n",
    "    cmp_graph.eval()\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings = np.zeros((timing_repeat, timing_number))\n",
    "    input_size, input_device = dummy_input.size(), dummy_input.device\n",
    "    sample_inputs = [torch.rand(input_size).to(input_device) for _ in range(100)]\n",
    "    timing_mean, timing_std = None, None\n",
    "\n",
    "    # GPU Warmup\n",
    "    for _ in range(10):\n",
    "         _ = cmp_graph(dummy_input)\n",
    "    \n",
    "    while timing_mean is None or timing_std > timing_mean/10:\n",
    "        # Measure Performance\n",
    "        with torch.no_grad():\n",
    "            for i in range(timing_repeat):\n",
    "                for j in range(timing_number):\n",
    "                    torch.manual_seed(i+j)\n",
    "                    start.record()\n",
    "                    _ = cmp_graph(sample_inputs[(i+j)%len(sample_inputs)])\n",
    "                    end.record()\n",
    "                    # Wait for GPU synchronization\n",
    "                    torch.cuda.synchronize()\n",
    "                    timings[i][j] = start.elapsed_time(end)\n",
    "        \n",
    "        timing_mean = np.mean(np.mean(timings, axis=1))\n",
    "        timing_median = np.median(timings)\n",
    "        timing_std = np.mean(np.std(timings, axis=1, ddof=1))\n",
    "    \n",
    "    latency = {\"mean\": timing_mean, \"median\": timing_median, \"std\": timing_std}\n",
    "    print(\"Inference Latency: %s \\n\" % (latency))\n",
    "    return latency['mean']\n",
    "\n",
    "# test_latency(origin_model, sample_input)\n",
    "# test_latency(merge_half_conv_model, sample_input)\n",
    "# test_latency(merge_all_conv_model, sample_input)\n",
    "# test_latency(SA_model, sample_input)\n",
    "# test_latency(LC_wo_rule_model, sample_input)\n",
    "# test_latency(LC_w_rule_model, sample_input)\n",
    "\n",
    "test_latency_cuda(origin_model, sample_input)\n",
    "test_latency_cuda(merge_half_conv_model, sample_input)\n",
    "test_latency_cuda(merge_all_conv_model, sample_input)\n",
    "test_latency_cuda(SA_model, sample_input)\n",
    "test_latency_cuda(LC_wo_rule_model, sample_input)\n",
    "test_latency_cuda(LC_w_rule_model, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf3bf5-21f5-420b-9317-60f6ee8d98b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d29ac5-fc0d-4fc5-880c-850be5455e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class toy_two_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_model, self).__init__()\n",
    "        self.task1 = nn.Sequential(\n",
    "            # first batch (64)\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            # first batch (64)\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.task1(x)\n",
    "        out2 = self.task2(x)\n",
    "        return out1, out2\n",
    "\n",
    "class toy_two_merge_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_1, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_2, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            # second batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_3, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_4, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            # Third Batch (256)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_5, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_6, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            # 4-th Batch (512)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_7, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # 5-th Batch (512)\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "\n",
    "class toy_two_merge_8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_8, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_9, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_10, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_11, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_12(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_12, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2\n",
    "    \n",
    "class toy_two_merge_13(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_two_merge_13, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        out1 = self.task1(shared)\n",
    "        out2 = self.task2(shared)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dfbed5c-8f3d-46f5-b68b-723e153a482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: {'mean': 1.56933633895177, 'median': 1.5661260995936268, 'std': 0.009906655276426849} \n",
      "\n",
      "Inference Latency: {'mean': 1.5143966564867233, 'median': 1.5084799528121948, 'std': 0.02323575368557845} \n",
      "\n",
      "Inference Latency: {'mean': 1.3858606990178426, 'median': 1.381376028060913, 'std': 0.016657756990358076} \n",
      "\n",
      "Inference Latency: {'mean': 1.3851025835673014, 'median': 1.3813120126724243, 'std': 0.016755386960390327} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3851025835673014"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metamorph.metrics.testing_utils import test_latency, test_latency_cuda\n",
    "\n",
    "# model = toy_two_model().eval().to(DEVICE)\n",
    "model = toy_two_merge_13().eval().to(DEVICE)\n",
    "sample_input = torch.rand(1,1,48,48).to(DEVICE)\n",
    "\n",
    "test_latency(model, sample_input)\n",
    "test_latency_cuda(model, sample_input)\n",
    "test_latency_cuda(model, sample_input)\n",
    "test_latency_cuda(model, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03185f2-727b-47d5-879b-81c6f203555e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tvm-build]",
   "language": "python",
   "name": "conda-env-.conda-tvm-build-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
