from pathlib import Path

import torch
from datasets import load_dataset
from transformers import AutoTokenizer


kwargs = {'num_workers': 4, 'pin_memory': True}
bs = 8
seed = 10
torch.manual_seed(seed)

cola_model_path = Path("cola/checkpoint")
cola_tokenizer = AutoTokenizer.from_pretrained(cola_model_path)

def preprocess_function(text, tokenizer=cola_tokenizer):
    # Tokenize the texts
    return tokenizer(text, padding="max_length", max_length=128, truncation=True, return_tensors="pt")

def collate_fn(examples):
    sentences = [preprocess_function(example['sentence']) for example in examples]
    labels = torch.tensor([example['label'] for example in examples])
    input_ids = torch.stack([sentence['input_ids'] for sentence in sentences])
    token_type_ids = torch.stack([sentence['token_type_ids'] for sentence in sentences])
    attention_mask = torch.stack([sentence['attention_mask'] for sentence in sentences])
    return {"input_ids": input_ids.squeeze(), 
            "token_type_ids": token_type_ids.squeeze(),
            "attention_mask": attention_mask.squeeze(),
            "labels": labels}

def collate_fn_sampler(examples): # for the dataset generated by GMorph's dataloader
    sentences = [preprocess_function(example[0]) for example in examples]
    input_ids = torch.stack([sentence['input_ids'] for sentence in sentences])
    token_type_ids = torch.stack([sentence['token_type_ids'] for sentence in sentences])
    attention_mask = torch.stack([sentence['attention_mask'] for sentence in sentences])
    dataset_index = [example[1] for example in examples]
    labels = [torch.stack([example[2][0] for example in examples]),
              torch.stack([example[2][1] for example in examples])]
    return ({"input_ids": input_ids.squeeze(), 
            "token_type_ids": token_type_ids.squeeze(), 
            "attention_mask": attention_mask.squeeze()}, 
            dataset_index, labels)

def load_data(task, bs=bs, kwargs=kwargs):
    raw_datasets = load_dataset("glue", task)
    train_dataset = raw_datasets['train']
    test_dataset = raw_datasets['validation']

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True, collate_fn=collate_fn, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=False, collate_fn=collate_fn, **kwargs)

    return train_loader, test_loader

def load_data_sampler(task):
    raw_datasets = load_dataset("glue", task)
    train_dataset = raw_datasets['train']
    return train_dataset